### âœ¨Rosalind Fokâœ¨
<!--
**RosalindFok/RosalindFok** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
#### Education
Bachelor of Engineering(computer science and technology): School of Computer Science and Technology, University of Chinese Academy of Sciences. My thesis was advised by [Beihong Jin](http://work.iscas.ac.cn/index.php/Jinbeihong/index/index)<br>
Master of Philosophy(computer science and technology): Center for Biomedical Information Technology, Institute of Advanced Computing and Digital Engineering, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences. I was advised by [Yunpeng Cai](https://szs.siat.ac.cn/#/detail?item=%5Bobject%20Object%5D&list=%5Bobject%20Object%5D&contentId=199)


#### Current Interests
Encoding and Decoding for higher cognitive function of human's brain.<br>
Incomplete understanding of the pathophysiology of mental disorders.<br> 
Graph Embedding and Geometry Learning.


#### Blogs
[äººå·¥æ™ºèƒ½èµ‹èƒ½è‡ªç„¶ç§‘å­¦(AI for Science, AI4Sci)çš„ç§‘ç ”æ–¹æ³•æ‚è°ˆ](https://zhuanlan.zhihu.com/p/651310815)<br>
[æ•°å­—å›¾åƒå¤„ç†ä¸­çš„äºŒç»´ç¦»æ•£å‚…é‡Œå¶å˜æ¢çš„æ€§è´¨](https://zhuanlan.zhihu.com/p/675724072)<br>

#### Published
[1] Yufu HUO,Beihong JIN,Zhaoyi LIAO. Multi-modal information augmented model for micro-video recommendation. Journal of ZheJiang University (Engineering Science), 2024, 58(6): 1142-1152.
**Abstract:** A multi-modal augmented model for click through rate (MMa4CTR) tailored for micro-videos recommendation was proposed. Multi-modal data derived from user interactions with micro-videos were effectively leveraged to construct embedded user representations and capture diverse user interests across multi-modal. The aim was to reveal the latent semantic commonalities, by combining and crossing features across modalities. The overall recommendation performance was boosted via two training strategies, automatic learning rate adjustment and validation interruption. A computationally efficient multi-layer perceptron architecture was employed, in order to address the computational demands brought on by the vast amount of multi-modal data. Performance comparison experiments and sensitivity analyses of hyperparameter on WeChat Video Channel and TikTok datasets demonstrated that MMa4CTR outperformed baseline models, delivering superior recommendation results with minimal computational resources. Additionally, ablation studies performed on both datasets further validated the significance and efficacy of the micro-video modality cross module, the user multi-modal embedding layer, and the strategies for automatic learning rate adjustment and validation interruption in enhancing recommendation performance.
**Key words:** recommender system    click through rate    multi modal    micro-video    machine learning
**Link:** https://www.zjujournals.com/eng/CN/Y2024/V58/I6/1142


![Dusai's GitHub stats](https://github-readme-stats.vercel.app/api?username=RosalindFok&show_icons=true&theme=radical)
![Rosalind Fok's Most used languages](https://github-readme-stats.vercel.app/api/top-langs?username=RosalindFok&show_icons=true&count_private=true&theme=gotham)
